#!/bin/bash

set -e

newer_cert() {
        local source_prefix=$${1}
        local dest_prefix=$${2:-$${1}}
        local target_suffix=$${3:-crt}
        local pki_dir=/etc/kubernetes/pki

        for word in $${target_suffix} key
        do
                if ! sudo diff $${source_prefix}.$${word} $${pki_dir}/$${dest_prefix}.$${word}
                then
                        return 0
                fi
        done
        return 1
}


# Retries a command on failure.
# $1 - the max number of attempts
# $2... - the command to run
retry() {
    local max_attempts="$${1}"; shift
    local attempt_num=1

    until "$${@}"
    do
        if [ "$${attempt_num}" -eq "$${max_attempts}" ]
        then
            echo "Attempt $${attempt_num} failed and there are no more attempts l
eft!"
            return 1
        else
            echo "Attempt $${attempt_num} failed! Trying again in $${attempt_num}
seconds..."
            sleep $(( attempt_num=attempt_num + 1 ))
        fi
    done
}

install_ca() {
        echo "Installing Updated CA"
        local pki_dir=/etc/kubernetes/pki
        sudo mkdir -p "$${pki_dir}/etcd"
        sudo install --compare --mode=0600 -D "$${HOME}/ca.key" "$${pki_dir}/ca.key"
        sudo install --compare --mode=0644 -D "$${HOME}/ca.crt" "$${pki_dir}/ca.crt"
        rm "$${HOME}/ca.key"
}

install() {
        install_ca

        echo "Running kubeadm init"
        sudo kubeadm init --upload-certs --config $HOME/install/kubeadm.conf

        ensure_certs_in_kubelet_config
        retry 5 sudo systemctl restart kubelet
        ensure_admin_config
        ensure_ancillary_items
}

ensure_updated_ca() {
        if newer_cert ca
        then
                install_ca
                update_cert_configs
                echo "CA CERTS UPDATED - NODES NEED RESTARTING"
        fi
}

update_cert_configs() {
        sudo mv /etc/kubernetes/*.conf /tmp
        sudo kubeadm init phase kubeconfig all --config $HOME/install/kubeadm.conf
        sudo kubeadm alpha certs renew all --config $HOME/install/kubeadm.conf
        sudo kubeadm init phase bootstrap-token --config $HOME/install/kubeadm.conf
        sudo kubeadm init phase upload-config all --config $HOME/install/kubeadm.conf
}

ensure_admin_config() {
        if [ -e /etc/kubernetes/admin.conf ]
        then
                echo "Installing admin config"
                mkdir -p $HOME/.kube
                sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
                sudo chown $(id -u):$(id -g) $HOME/.kube/config
        fi

        echo "Installing client cert generator"
        cat <<"CLIENTSCRIPT" | sudo tee /usr/local/bin/generate-client-cert.sh
#!/usr/bin/env bash

if [[ "$${#}" -lt 2 ]]; then
    echo "Usage: $${0} user group [group ...]"
    exit 1
fi

USER=$${1}; shift
GROUP=""
for word
do
  GROUP="$${GROUP}/O=$${word}"
done
CLUSTERENDPOINT=https://${public_fqdn}:${service_port}
CLUSTERNAME=${cluster_fqdn}
CACERT=/etc/kubernetes/pki/ca.crt
CAKEY=/etc/kubernetes/pki/ca.key
CLIENTCERTKEY=clients/$${USER}/$${USER}.key
CLIENTCERTCSR=clients/$${USER}/$${USER}.csr
CLIENTCERTCRT=clients/$${USER}/$${USER}.crt

if [ ! -r "$${CAKEY}" ]; then
    echo "Insufficent permissions to read $${CAKEY}"
    exit 1
fi

mkdir -p clients/$${USER}

openssl genrsa -out $${CLIENTCERTKEY} 4096
openssl req -new -key $${CLIENTCERTKEY} -out $${CLIENTCERTCSR} \
      -subj "/CN=$${USER}$${GROUP}"
openssl x509 -req -days 365 -sha256 -in $${CLIENTCERTCSR} -CA $${CACERT} -CAkey $${CAKEY} -set_serial 2 -out $${CLIENTCERTCRT}

cat <<-EOF > clients/$${USER}/kubeconfig
apiVersion: v1
kind: Config
preferences:
  colors: true
current-context: $${CLUSTERNAME}
clusters:
- name: $${CLUSTERNAME}
  cluster:
    server: $${CLUSTERENDPOINT}
    certificate-authority-data: |-
      $(base64 --wrap=0 $${CACERT})
contexts:
- context:
    cluster: $${CLUSTERNAME}
    user: $${USER}
  name: $${CLUSTERNAME}
users:
- name: $${USER}
  user:
    client-certificate-data: |-
      $(base64 --wrap=0 $${CLIENTCERTCRT})
    client-key-data: |-
      $(base64 --wrap=0 $${CLIENTCERTKEY})
EOF
CLIENTSCRIPT
        echo "Setting permissions"
        sudo chmod 755 /usr/local/bin/generate-client-cert.sh
}

ensure_ancillary_items() {
        echo "Patching kube-proxy scheduling tolerations"
        if ! kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "metadata": { "annotations": { "scheduler.alpha.kubernetes.io/critical-pod": "" } }, "spec": { "priorityClassName": "system-node-critical", "tolerations": [{ "key": "CriticalAddonsOnly", "operator": "Exists" }, { "operator": "Exists" }] } } } }'
        then
                echo "Patch already applied - skipping"
        fi

        echo "Installing Calico network controllers"
        retry 5 kubectl apply -f \
        https://docs.projectcalico.org/v${calico_release}/manifests/calico.yaml

        case "${storage_system}" in
            manual)
                ;;
            openebs)
                echo "Installing OpenEBS storage controllers"
                retry 5 kubectl apply -f \
                    https://openebs.github.io/charts/openebs-operator.yaml
                ;;
            *) 
                echo "Unknown storage system ${storage_system}"
                ;;
        esac

        if [ "${manage_autoscaler}" = "true" ]
        then
            echo "Requesting Vertical Autoscaler activation"
            install_autoscaler
        fi
}

ensure_certs_in_kubelet_config() {
        if sudo test -f /var/lib/kubelet/pki/kubelet-client-current.pem
        then
                sudo sed -i 's/client-certificate-data: .*$/client-certificate: \/var\/lib\/kubelet\/pki\/kubelet-client-current.pem/
                        s/client-key-data: .*$/client-key: \/var\/lib\/kubelet\/pki\/kubelet-client-current.pem/' /etc/kubernetes/kubelet.conf
        fi
}

ensure_docker_settings() {
        if [ ! -f /etc/docker/daemon.json ]
        then
                echo "Writing Updated Docker Configuration"
                cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "log-driver": "json-file",
  "live-restore": true,
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
                sudo systemctl reload docker
                sudo mkdir -p /etc/systemd/system/docker.service.d
                sudo systemctl daemon-reload
        fi
}

install_autoscaler(){
        cat <<EOF > $HOME/install/autoscaler.yml
---
# Source: cluster-autoscaler/templates/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: "release"
      app.kubernetes.io/name: "brightbox-cluster-autoscaler"
  maxUnavailable: 1
---
# Source: cluster-autoscaler/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
---
# Source: cluster-autoscaler/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
rules:
  - apiGroups:
      - ""
    resources:
      - events
      - endpoints
    verbs:
      - create
      - patch
  - apiGroups:
    - ""
    resources:
    - pods/eviction
    verbs:
    - create
  - apiGroups:
      - ""
    resources:
      - pods/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - endpoints
    resourceNames:
      - cluster-autoscaler
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
    - watch
    - list
    - get
    - update
  - apiGroups:
    - ""
    resources:
      - pods
      - services
      - replicationcontrollers
      - persistentvolumeclaims
      - persistentvolumes
    verbs:
      - watch
      - list
      - get
  - apiGroups:
    - batch
    resources:
      - jobs
      - cronjobs
    verbs:
      - watch
      - list
      - get
  - apiGroups:
    - batch
    - extensions
    resources:
    - jobs
    verbs:
    - get
    - list
    - patch
    - watch
  - apiGroups:
      - extensions
    resources:
      - replicasets
      - daemonsets
    verbs:
      - watch
      - list
      - get
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - watch
      - list
  - apiGroups:
    - apps
    resources:
    - daemonsets
    - replicasets
    - statefulsets
    verbs:
    - watch
    - list
    - get
  - apiGroups:
    - storage.k8s.io
    resources:
    - storageclasses
    - csinodes
    verbs:
    - watch
    - list
    - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - list
      - watch
  - apiGroups:
    - coordination.k8s.io
    resources:
    - leases
    verbs:
    - create
  - apiGroups:
    - coordination.k8s.io
    resourceNames:
    - cluster-autoscaler
    resources:
    - leases
    verbs:
    - get
    - update
---
# Source: cluster-autoscaler/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-brightbox-cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: release-brightbox-cluster-autoscaler
    namespace: kube-system
---
# Source: cluster-autoscaler/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - cluster-autoscaler-status
    verbs:
      - delete
      - get
      - update
---
# Source: cluster-autoscaler/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-brightbox-cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: release-brightbox-cluster-autoscaler
    namespace: kube-system
---
# Source: cluster-autoscaler/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
spec:
  ports:
    - port: 8085
      protocol: TCP
      targetPort: 8085
      name: http
  selector:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
  type: "ClusterIP"
---
# Source: cluster-autoscaler/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: "release"
      app.kubernetes.io/name: "brightbox-cluster-autoscaler"
  template:
    metadata:
      annotations:
        prometheus.io/port: "8085"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/instance: "release"
        app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    spec:
      priorityClassName: "system-cluster-critical"
      dnsPolicy: "Default"
      containers:
        - name: brightbox-cluster-autoscaler
          image: "brightbox/cluster-autoscaler-brightbox:${autoscaler_release}"
          imagePullPolicy: "Always"
          command:
            - ./cluster-autoscaler
            - --cloud-provider=brightbox
            - --namespace=kube-system
            - --cluster-name=${cluster_fqdn}
            - --logtostderr=true
            - --skip-nodes-with-local-storage=true
            - --stderrthreshold=info
            - --v=2

          env:
          envFrom:
            - secretRef:
                name: brightbox-credentials
          livenessProbe:
            httpGet:
              path: /health-check
              port: 8085
          ports:
            - containerPort: 8085
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
      serviceAccountName: release-brightbox-cluster-autoscaler
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - key: CriticalAddonsOnly
          operator: Exists
EOF
        retry 5 kubectl -n kube-system apply -f $HOME/install/autoscaler.yml
}

ensure_updated_apt_keys() {
    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
}

upgrade() {
        ensure_updated_ca
        ensure_updated_apt_keys

        if [ -f /var/run/docker.sock ]
        then
                ensure_docker_settings
        fi
        #echo "Restarting docker service to ensure correct cgroup"
        #sudo systemctl restart docker
        echo "Kubeadm installed master detected - checking for upgrade"
        sudo apt-get -qq update -y
        sudo apt-get -qq install -y  debconf-utils
        echo "docker.io docker.io/restart select true" | sudo debconf-set-selections
        if ! retry 5 sudo apt-get -qq -y --allow-change-held-packages install kubeadm=${kubernetes_release}-00
        then
                sudo apt-mark hold kubeadm
                echo "Failed to install kubeadm at version ${kubernetes_release}-00"
                echo "Version installed is at $(dpkg-query --showformat='$${Version}' --show kubeadm)"
                return 2
        fi
        sudo apt-mark hold kubeadm
	plan=$(sudo kubeadm upgrade plan ${kubernetes_release})
	case "$${plan}" in
	*"kubeadm upgrade apply"*)
                retry 5 sudo kubeadm upgrade apply -y ${kubernetes_release}
                echo "Upgrading kubelet"
                retry 5 sudo apt-get -qq -y --allow-change-held-packages install \
                        kubectl=${kubernetes_release}-00 \
                        kubelet=${kubernetes_release}-00
                sudo apt-mark hold kubelet kubectl
                ensure_certs_in_kubelet_config
                retry 5 sudo systemctl restart kubelet
		;;
    *)  echo "Already at version ${kubernetes_release} or greater - refreshing certificates"
		echo $${plan}
        retry 5 sudo kubeadm init phase upload-certs --upload-certs --config $HOME/install/kubeadm.conf
		;;
	esac
    ensure_admin_config
    ensure_ancillary_items
}

if ss -lnt | grep -q :6443
then
        upgrade
else
        install
fi

#!/bin/bash

set -e

newer_cert() {
        local source_prefix=$${1}
        local dest_prefix=$${2:-$${1}}
        local target_suffix=$${3:-crt}
        local pki_dir=/etc/kubernetes/pki

        for word in $${target_suffix} key
        do
                if sudo test $${source_prefix}.$${word} -nt $${pki_dir}/$${dest_prefix}.$${word}
                then
                        return 0
                fi
        done
        return 1
}


# Retries a command on failure.
# $1 - the max number of attempts
# $2... - the command to run
retry() {
    local max_attempts="$${1}"; shift
    local attempt_num=1

    until "$${@}"
    do
        if [ "$${attempt_num}" -eq "$${max_attempts}" ]
        then
            echo "Attempt $${attempt_num} failed and there are no more attempts l
eft!"
            return 1
        else
            echo "Attempt $${attempt_num} failed! Trying again in $${attempt_num}
seconds..."
            sleep $(( attempt_num=attempt_num + 1 ))
        fi
    done
}

install_ca() {
        echo "Installing Updated CA"
        local pki_dir=/etc/kubernetes/pki
        sudo mkdir -p "$${pki_dir}/etcd"
        sudo cp "$${HOME}/ca.crt" "$${HOME}/ca.key" "$${pki_dir}"
        sudo chmod 0600 "$${pki_dir}/ca.key"
        rm "$${HOME}/ca.key"
}

install() {
        install_ca

        echo "Running kubeadm init"
        sudo kubeadm init --upload-certs --config $HOME/install/kubeadm.conf

        ensure_certs_in_kubelet_config
        retry 5 sudo systemctl restart kubelet
        ensure_admin_config
        ensure_ancillary_items
}

ensure_updated_ca() {
        if newer_cert ca
        then
                install_ca
                update_cert_configs
                echo "CA CERTS UPDATED - NODES NEED RESTARTING"
        fi
}

update_cert_configs() {
        sudo mv /etc/kubernetes/*.conf /tmp
        sudo kubeadm init phase kubeconfig all --config $HOME/install/kubeadm.conf
        sudo kubeadm alpha certs renew all --config $HOME/install/kubeadm.conf
        sudo kubeadm init phase bootstrap-token --config $HOME/install/kubeadm.conf
        sudo kubeadm init phase upload-config all --config $HOME/install/kubeadm.conf
}

ensure_admin_config() {
        if [ -e /etc/kubernetes/admin.conf ]
        then
                echo "Installing admin config"
                mkdir -p $HOME/.kube
                sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
                sudo chown $(id -u):$(id -g) $HOME/.kube/config
        fi

        echo "Installing client cert generator"
        cat <<"CLIENTSCRIPT" | sudo tee /usr/local/bin/generate-client-cert.sh
#!/usr/bin/env bash

if [[ "$${#}" -lt 2 ]]; then
    echo "Usage: $${0} user group [group ...]"
    exit 1
fi

USER=$${1}; shift
GROUP=""
for word
do
  GROUP="$${GROUP}/O=$${word}"
done
CLUSTERENDPOINT=https://${public_fqdn}:${service_port}
CLUSTERNAME=${cluster_fqdn}
CACERT=/etc/kubernetes/pki/ca.crt
CAKEY=/etc/kubernetes/pki/ca.key
CLIENTCERTKEY=clients/$${USER}/$${USER}.key
CLIENTCERTCSR=clients/$${USER}/$${USER}.csr
CLIENTCERTCRT=clients/$${USER}/$${USER}.crt

if [ ! -r "$${CAKEY}" ]; then
    echo "Insufficent permissions to read $${CAKEY}"
    exit 1
fi

mkdir -p clients/$${USER}

openssl genrsa -out $${CLIENTCERTKEY} 4096
openssl req -new -key $${CLIENTCERTKEY} -out $${CLIENTCERTCSR} \
      -subj "/CN=$${USER}$${GROUP}"
openssl x509 -req -days 365 -sha256 -in $${CLIENTCERTCSR} -CA $${CACERT} -CAkey $${CAKEY} -set_serial 2 -out $${CLIENTCERTCRT}

cat <<-EOF > clients/$${USER}/kubeconfig
apiVersion: v1
kind: Config
preferences:
  colors: true
current-context: $${CLUSTERNAME}
clusters:
- name: $${CLUSTERNAME}
  cluster:
    server: $${CLUSTERENDPOINT}
    certificate-authority-data: |-
      $(base64 --wrap=0 $${CACERT})
contexts:
- context:
    cluster: $${CLUSTERNAME}
    user: $${USER}
  name: $${CLUSTERNAME}
users:
- name: $${USER}
  user:
    client-certificate-data: |-
      $(base64 --wrap=0 $${CLIENTCERTCRT})
    client-key-data: |-
      $(base64 --wrap=0 $${CLIENTCERTKEY})
EOF
CLIENTSCRIPT
        echo "Setting permissions"
        sudo chmod 755 /usr/local/bin/generate-client-cert.sh
}

controller_key_name=brightbox-credentials
apiurl_b64=$(echo -n ${apiurl} | base64)
controller_client_b64=$(echo -n ${controller_client} | base64)
controller_client_secret_b64=$(echo -n ${controller_client_secret} | base64)
kubernetes_release_b64=$(echo -n ${kubernetes_release} | base64)


ensure_ancillary_items() {
        echo "Patching kube-proxy scheduling tolerations"
        if ! kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "metadata": { "annotations": { "scheduler.alpha.kubernetes.io/critical-pod": "" } }, "spec": { "priorityClassName": "system-node-critical", "tolerations": [{ "key": "CriticalAddonsOnly", "operator": "Exists" }, { "operator": "Exists" }] } } } }'
        then
                echo "Patch already applied - skipping"
        fi

        echo "Creating boot token"
        kubeadm token delete "${boot_token}" || true
        join_command=$(kubeadm token create "${boot_token}" --ttl 0 --description 'Cluster autoscaling token' --print-join-command 2>/dev/null | tr -d '\n\r' | base64 --wrap=0)

        echo "Requesting Brightbox cloud controller activation"
        install_cloud_controller

        echo "Installing Calico network controllers"
        retry 5 kubectl apply -f \
        https://docs.projectcalico.org/v${calico_release}/manifests/calico.yaml

        case "${storage_system}" in
            manual)
                ;;
            openebs)
                echo "Installing OpenEBS storage controllers"
                retry 5 kubectl apply -f \
                    https://openebs.github.io/charts/openebs-operator.yaml
                ;;
            *) 
                echo "Unknown storage system ${storage_system}"
                ;;
        esac

        if [ "${manage_autoscaler}" = "true" ]
        then
            echo "Requesting Vertical Autoscaler activation"
            install_autoscaler
        fi
}

ensure_certs_in_kubelet_config() {
        if sudo test -f /var/lib/kubelet/pki/kubelet-client-current.pem
        then
                sudo sed -i 's/client-certificate-data: .*$/client-certificate: \/var\/lib\/kubelet\/pki\/kubelet-client-current.pem/
                        s/client-key-data: .*$/client-key: \/var\/lib\/kubelet\/pki\/kubelet-client-current.pem/' /etc/kubernetes/kubelet.conf
        fi
}

ensure_docker_settings() {
        if [ ! -f /etc/docker/daemon.json ]
        then
                echo "Writing Updated Docker Configuration"
                cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "log-driver": "json-file",
  "live-restore": true,
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
                sudo systemctl reload docker
                sudo mkdir -p /etc/systemd/system/docker.service.d
                sudo systemctl daemon-reload
        fi
}

install_cloud_controller() {
        cat <<EOF > $HOME/install/cloud-controller.yml
---
apiVersion: v1
data:
  BRIGHTBOX_API_URL: $${apiurl_b64}
  BRIGHTBOX_CLIENT: $${controller_client_b64}
  BRIGHTBOX_CLIENT_SECRET: $${controller_client_secret_b64}
  BRIGHTBOX_KUBE_JOIN_COMMAND: $${join_command}
  BRIGHTBOX_KUBE_VERSION: $${kubernetes_release_b64}
kind: Secret
metadata:
  name: $${controller_key_name}
  namespace: kube-system
type: Opaque
---
apiVersion: v1
items:
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: system:cloud-controller-manager
  rules:
  - apiGroups:
    - coordination.k8s.io
    resources:
    - leases
    verbs:
    - get
    - create
    - update
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - '*'
  - apiGroups:
    - ""
    resources:
    - nodes/status
    verbs:
    - patch
  - apiGroups:
    - ""
    resources:
    - services
    verbs:
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - serviceaccounts
    verbs:
    - create
    - get
  - apiGroups:
    - ""
    resources:
    - persistentvolumes
    verbs:
    - '*'
  - apiGroups:
    - ""
    resources:
    - endpoints
    verbs:
    - create
    - get
    - list
    - watch
    - update
  - apiGroups:
    - ""
    resources:
    - configmaps
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - secrets
    verbs:
    - list
    - get
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: system:cloud-node-controller
  rules:
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - '*'
  - apiGroups:
    - ""
    resources:
    - nodes/status
    verbs:
    - patch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: system:pvl-controller
  rules:
  - apiGroups:
    - ""
    resources:
    - persistentvolumes
    verbs:
    - '*'
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
kind: List
metadata: {}
---
apiVersion: v1
items:
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: system:cloud-node-controller
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:cloud-node-controller
  subjects:
  - kind: ServiceAccount
    name: cloud-node-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: system:pvl-controller
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:pvl-controller
  subjects:
  - kind: ServiceAccount
    name: pvl-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: system:cloud-controller-manager
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:cloud-controller-manager
  subjects:
  - kind: ServiceAccount
    name: cloud-controller-manager
    namespace: kube-system
kind: List
metadata: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: brightbox-cloud-controller-manager
  name: brightbox-cloud-controller-manager
  namespace: kube-system
data:
  cloud-controller.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://[${public_ip}]:${service_port}
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cloud-controller-manager
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    k8s-app: brightbox-cloud-controller-manager
  name: brightbox-cloud-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: brightbox-cloud-controller-manager
  template:
    metadata:
      labels:
        k8s-app: brightbox-cloud-controller-manager
      annotations: {
        "apiclientid": "${controller_client}"
      }
    spec:
      nodeSelector:
        node-role.kubernetes.io/master: ""
      securityContext:
        runAsUser: 1001
      dnsPolicy: Default
      tolerations:
        # this taint is set by all kubelets running '--cloud-provider=external'
        # so we should tolerate it to schedule the brightbox ccm
        - key: "node.cloudprovider.kubernetes.io/uninitialized"
          value: "true"
          effect: "NoSchedule"
        - key: "node-role.kubernetes.io/master"
          effect: NoSchedule
        - key: "CriticalAddonsOnly"
          operator: "Exists"
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
      serviceAccountName: cloud-controller-manager
      containers:
      - name: brightbox-cloud-controller-manager
        imagePullPolicy: Always
        image: brightbox/brightbox-cloud-controller-manager:${kubernetes_release}
        args:
          - "--cloud-provider=brightbox"
          - "--bind-address=${local_host}"
          - "--secure-port=10258"
          - "--configure-cloud-routes=false"
          - "--kubeconfig=/etc/kubernetes/cloud-controller.conf"
          - "--cluster-name=${cluster_fqdn}"
          - "--use-service-account-credentials=true"
        resources:
          requests:
            cpu: 200m
        envFrom:
        - secretRef:
            name: $${controller_key_name}
        env:
          - name: KUBERNETES_SERVICE_HOST
            value: "${public_ip}"
          - name: KUBERNETES_SERVICE_PORT
            value: "${service_port}"
        volumeMounts:
        - mountPath: /etc/kubernetes/cloud-controller.conf
          name: cloud-controller-conf
          readOnly: true
          subPath: cloud-controller.conf
      hostNetwork: true
      volumes:
      - configMap:
          defaultMode: 420
          name: brightbox-cloud-controller-manager
        name: cloud-controller-conf
EOF
        retry 5 kubectl apply -f $HOME/install/cloud-controller.yml
}

install_autoscaler(){
        cat <<EOF > $HOME/install/autoscaler.yml
---
# Source: cluster-autoscaler/templates/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: "release"
      app.kubernetes.io/name: "brightbox-cluster-autoscaler"
  maxUnavailable: 1
---
# Source: cluster-autoscaler/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
---
# Source: cluster-autoscaler/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
rules:
  - apiGroups:
      - ""
    resources:
      - events
      - endpoints
    verbs:
      - create
      - patch
  - apiGroups:
    - ""
    resources:
    - pods/eviction
    verbs:
    - create
  - apiGroups:
      - ""
    resources:
      - pods/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - endpoints
    resourceNames:
      - cluster-autoscaler
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
    - watch
    - list
    - get
    - update
  - apiGroups:
    - ""
    resources:
      - pods
      - services
      - replicationcontrollers
      - persistentvolumeclaims
      - persistentvolumes
    verbs:
      - watch
      - list
      - get
  - apiGroups:
    - batch
    resources:
      - jobs
      - cronjobs
    verbs:
      - watch
      - list
      - get
  - apiGroups:
    - batch
    - extensions
    resources:
    - jobs
    verbs:
    - get
    - list
    - patch
    - watch
  - apiGroups:
      - extensions
    resources:
      - replicasets
      - daemonsets
    verbs:
      - watch
      - list
      - get
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - watch
      - list
  - apiGroups:
    - apps
    resources:
    - daemonsets
    - replicasets
    - statefulsets
    verbs:
    - watch
    - list
    - get
  - apiGroups:
    - storage.k8s.io
    resources:
    - storageclasses
    - csinodes
    verbs:
    - watch
    - list
    - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - list
      - watch
  - apiGroups:
    - coordination.k8s.io
    resources:
    - leases
    verbs:
    - create
  - apiGroups:
    - coordination.k8s.io
    resourceNames:
    - cluster-autoscaler
    resources:
    - leases
    verbs:
    - get
    - update
---
# Source: cluster-autoscaler/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-brightbox-cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: release-brightbox-cluster-autoscaler
    namespace: kube-system
---
# Source: cluster-autoscaler/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - cluster-autoscaler-status
    verbs:
      - delete
      - get
      - update
---
# Source: cluster-autoscaler/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-brightbox-cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: release-brightbox-cluster-autoscaler
    namespace: kube-system
---
# Source: cluster-autoscaler/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
spec:
  ports:
    - port: 8085
      protocol: TCP
      targetPort: 8085
      name: http
  selector:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
  type: "ClusterIP"
---
# Source: cluster-autoscaler/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    app.kubernetes.io/managed-by: "Helm"
    helm.sh/chart: "cluster-autoscaler-7.3.2"
  name: release-brightbox-cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: "release"
      app.kubernetes.io/name: "brightbox-cluster-autoscaler"
  template:
    metadata:
      annotations:
        prometheus.io/port: "8085"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/instance: "release"
        app.kubernetes.io/name: "brightbox-cluster-autoscaler"
    spec:
      priorityClassName: "system-cluster-critical"
      dnsPolicy: "Default"
      containers:
        - name: brightbox-cluster-autoscaler
          image: "brightbox/cluster-autoscaler-brightbox:${autoscaler_release}"
          imagePullPolicy: "Always"
          command:
            - ./cluster-autoscaler
            - --cloud-provider=brightbox
            - --namespace=kube-system
            - --cluster-name=${cluster_fqdn}
            - --logtostderr=true
            - --skip-nodes-with-local-storage=true
            - --stderrthreshold=info
            - --v=2

          env:
          envFrom:
            - secretRef:
                name: brightbox-credentials
          livenessProbe:
            httpGet:
              path: /health-check
              port: 8085
          ports:
            - containerPort: 8085
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
      serviceAccountName: release-brightbox-cluster-autoscaler
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - key: CriticalAddonsOnly
          operator: Exists
EOF
        retry 5 kubectl -n kube-system apply -f $HOME/install/autoscaler.yml
}

upgrade() {
        ensure_updated_ca

        if [ -f /var/run/docker.sock ]
        then
                ensure_docker_settings
        fi
        #echo "Restarting docker service to ensure correct cgroup"
        #sudo systemctl restart docker
        echo "Kubeadm installed master detected - checking for upgrade"
        sudo apt-get -qq update -y
        sudo apt-get -qq install -y  debconf-utils
        echo "docker.io docker.io/restart select true" | sudo debconf-set-selections
        sudo apt-mark unhold kubeadm
        if ! retry 5 sudo apt-get -qq install -y kubeadm=${kubernetes_release}-00
        then
                sudo apt-mark hold kubeadm
                echo "Failed to install kubeadm at version ${kubernetes_release}-00"
                echo "Version installed is at $(dpkg-query --showformat='$${Version}' --show kubeadm)"
                return 2
        fi
        sudo apt-mark hold kubeadm
	plan=$(sudo kubeadm upgrade plan ${kubernetes_release})
	case "$${plan}" in
	*"kubeadm upgrade apply"*)
                retry 5 sudo kubeadm upgrade apply -y ${kubernetes_release}
                echo "Upgrading kubelet"
                sudo apt-mark unhold kubelet kubectl
                retry 5 sudo apt-get -qq -y install \
                        kubectl=${kubernetes_release}-00 \
                        kubelet=${kubernetes_release}-00
                sudo apt-mark hold kubelet kubectl
                ensure_certs_in_kubelet_config
                retry 5 sudo systemctl restart kubelet
		;;
	*up-to-date*)
		echo "Already at version ${kubernetes_release} or greater - refreshing certificates"
        retry 5 sudo kubeadm init phase upload-certs --upload-certs --config $HOME/install/kubeadm.conf
		;;
	*)
		echo "Unexpected plan response"
		echo $${plan}
		return 1
		;;
	esac
        ensure_admin_config
        ensure_ancillary_items
}

if ss -lnt | grep -q :6443
then
        upgrade
else
        install
fi

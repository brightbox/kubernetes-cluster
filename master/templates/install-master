#!/bin/bash

set -e


install_cert() {
        local source_prefix=$${1}
        local dest_prefix=$${2:-$${1}}
        local target_suffix=$${3:-crt}
        local pki_dir=/etc/kubernetes/pki

        for word in $${target_suffix} key
        do
                sudo cp $${source_prefix}.$${word} $${pki_dir}/$${dest_prefix}.$${word}
        done
        sudo chmod 0600 $${pki_dir}/$${dest_prefix}.key
        rm $${source_prefix}.key
}

newer_cert() {
        local source_prefix=$${1}
        local dest_prefix=$${2:-$${1}}
        local target_suffix=$${3:-crt}
        local pki_dir=/etc/kubernetes/pki

        for word in $${target_suffix} key
        do
                if sudo test $${source_prefix}.$${word} -nt $${pki_dir}/$${dest_prefix}.$${word}
                then
                        return 0
                fi
        done
        return 1
}


# Retries a command on failure.
# $1 - the max number of attempts
# $2... - the command to run
retry() {
    local max_attempts="$${1}"; shift
    local attempt_num=1

    until "$${@}"
    do
        if [ "$${attempt_num}" -eq "$${max_attempts}" ]
        then
            echo "Attempt $${attempt_num} failed and there are no more attempts l
eft!"
            return 1
        else
            echo "Attempt $${attempt_num} failed! Trying again in $${attempt_num}
seconds..."
            sleep $(( attempt_num=attempt_num + 1 ))
        fi
    done
}

install_ca() {
        echo "Installing Updated CA"
        sudo mkdir -p /etc/kubernetes/pki/etcd
        install_cert ca
}

install() {
        install_ca

        echo "Running kubeadm init"
        sudo kubeadm init --config $HOME/install/kubeadm.conf

        ensure_certs_in_kubelet_config
        retry 5 sudo systemctl restart kubelet
        ensure_admin_config
        ensure_ancillary_items
}

ensure_updated_ca() {
        if newer_cert ca
        then
                install_ca
                update_cert_configs
                echo "CA CERTS UPDATED - NODES NEED RESTARTING"
        fi
}

update_cert_configs() {
        sudo mv /etc/kubernetes/*.conf /tmp
        sudo kubeadm init phase kubeconfig all --config $HOME/install/kubeadm.conf
        sudo kubeadm alpha certs renew all --config $HOME/install/kubeadm.conf
        sudo kubeadm init phase bootstrap-token --config $HOME/install/kubeadm.conf
        sudo kubeadm init phase upload-config all --config $HOME/install/kubeadm.conf
}

ensure_admin_config() {
        if [ -e /etc/kubernetes/admin.conf ]
        then
                echo "Installing admin config"
                mkdir -p $HOME/.kube
                sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
                sudo chown $(id -u):$(id -g) $HOME/.kube/config
        fi

        echo "Installing client cert generator"
        cat <<"CLIENTSCRIPT" | sudo tee /usr/local/bin/generate-client-cert.sh
#!/usr/bin/env bash

if [[ "$${#}" -lt 2 ]]; then
    echo "Usage: $${0} user group [group ...]"
    exit 1
fi

USER=$${1}; shift
GROUP=""
for word
do
  GROUP="$${GROUP}/O=$${word}"
done
CLUSTERENDPOINT=https://${public_fqdn}:${service_port}
CLUSTERNAME=${cluster_name}
CACERT=/etc/kubernetes/pki/ca.crt
CAKEY=/etc/kubernetes/pki/ca.key
CLIENTCERTKEY=clients/$${USER}/$${USER}.key
CLIENTCERTCSR=clients/$${USER}/$${USER}.csr
CLIENTCERTCRT=clients/$${USER}/$${USER}.crt

if [ ! -r "$${CAKEY}" ]; then
    echo "Insufficent permissions to read $${CAKEY}"
    exit 1
fi

mkdir -p clients/$${USER}

openssl genrsa -out $${CLIENTCERTKEY} 4096
openssl req -new -key $${CLIENTCERTKEY} -out $${CLIENTCERTCSR} \
      -subj "/CN=$${USER}$${GROUP}"
openssl x509 -req -days 365 -sha256 -in $${CLIENTCERTCSR} -CA $${CACERT} -CAkey $${CAKEY} -set_serial 2 -out $${CLIENTCERTCRT}

cat <<-EOF > clients/$${USER}/kubeconfig
apiVersion: v1
kind: Config
preferences:
  colors: true
current-context: $${CLUSTERNAME}
clusters:
- name: $${CLUSTERNAME}
  cluster:
    server: $${CLUSTERENDPOINT}
    certificate-authority-data: |-
      $(base64 --wrap=0 $${CACERT})
contexts:
- context:
    cluster: $${CLUSTERNAME}
    user: $${USER}
  name: $${CLUSTERNAME}
users:
- name: $${USER}
  user:
    client-certificate-data: |-
      $(base64 --wrap=0 $${CLIENTCERTCRT})
    client-key-data: |-
      $(base64 --wrap=0 $${CLIENTCERTKEY})
EOF
CLIENTSCRIPT
        echo "Setting permissions"
        sudo chmod 755 /usr/local/bin/generate-client-cert.sh
}

controller_key_name=brightbox-credentials
apiurl_b64=$(echo -n ${apiurl} | base64)
controller_client_b64=$(echo -n ${controller_client} | base64)
controller_client_secret_b64=$(echo -n ${controller_client_secret} | base64)
boot_token_b64=$(echo -n ${boot_token} | base64)
ca_hash_b64=$(openssl x509 -noout -pubkey <<CA |openssl rsa -pubin -outform DER 2>/dev/null | sha256sum | cut -d' ' -f1 | tr -d '\n' | base64 --wrap=0
${certificate_authority_pem}
CA
)

ensure_ancillary_items() {
        echo "Patching kube-proxy scheduling tolerations"
        if ! kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "metadata": { "annotations": { "scheduler.alpha.kubernetes.io/critical-pod": "" } }, "spec": { "priorityClassName": "system-node-critical", "tolerations": [{ "key": "CriticalAddonsOnly", "operator": "Exists" }, { "operator": "Exists" }] } } } }'
        then
                echo "Patch already applied - skipping"
        fi

        echo "Requesting Brightbox cloud controller activation"
        cat <<EOF > $HOME/install/cloud-controller.yml
---
apiVersion: v1
data:
  BRIGHTBOX_API_URL: $${apiurl_b64}
  BRIGHTBOX_CLIENT: $${controller_client_b64}
  BRIGHTBOX_CLIENT_SECRET: $${controller_client_secret_b64}
  BRIGHTBOX_KUBE_BOOT_TOKEN: $${boot_token_b64}
  BRIGHTBOX_KUBE_CA_HASH: $${ca_hash_b64}
kind: Secret
metadata:
  name: $${controller_key_name}
  namespace: kube-system
type: Opaque
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cloud-controller-manager
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: system:cloud-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: cloud-controller-manager
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
data:
  cloud-controller.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://[${public_ip}]:${service_port}
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    k8s-app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cloud-controller-manager
  template:
    metadata:
      labels:
        k8s-app: cloud-controller-manager
      annotations: {
        "apiclientid": "${controller_client}"
      }
    spec:
      dnsPolicy: Default
      hostNetwork: true
      serviceAccountName: cloud-controller-manager
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
        # this taint is set by all kubelets running '--cloud-provider=external'
        # so we should tolerate it to schedule the brightbox ccm
        - key: "node.cloudprovider.kubernetes.io/uninitialized"
          value: "true"
          effect: "NoSchedule"
        - key: "CriticalAddonsOnly"
          operator: "Exists"
        - key: "node-role.kubernetes.io/master"
          effect: NoSchedule
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
      containers:
      - name: cloud-controller-manager
        # for in-tree providers we use k8s.gcr.io/cloud-controller-manager
        # this can be replaced with any other image for out-of-tree providers
        imagePullPolicy: Always
        image: brightbox/brightbox-cloud-controller-manager:${kubernetes_release}
        args:
          - "--cloud-provider=brightbox"
          - "--bind-address=${local_host}"
          - "--secure-port=10258"
          - "--configure-cloud-routes=false"
          - "--kubeconfig=/etc/kubernetes/cloud-controller.conf"
          - "--cluster-name=${cluster_name}"
          - "--leader-elect=true"
          - --use-service-account-credentials=false
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
        envFrom:
        - secretRef:
            name: $${controller_key_name}
        env:
          - name: KUBERNETES_SERVICE_HOST
            value: "${public_ip}"
          - name: KUBERNETES_SERVICE_PORT
            value: "${service_port}"
        volumeMounts:
        - mountPath: /etc/kubernetes/cloud-controller.conf
          name: cloud-controller-conf
          readOnly: true
          subPath: cloud-controller.conf
      hostNetwork: true
      volumes:
      - configMap:
          defaultMode: 420
          name: cloud-controller-manager
        name: cloud-controller-conf
EOF
        retry 5 kubectl apply -f $HOME/install/cloud-controller.yml

        echo "Installing Calico network controllers"
        retry 5 kubectl apply -f \
        https://docs.projectcalico.org/v${calico_release}/manifests/calico.yaml
}

ensure_certs_in_kubelet_config() {
        if sudo test -f /var/lib/kubelet/pki/kubelet-client-current.pem
        then
                sudo sed -i 's/client-certificate-data: .*$/client-certificate: \/var\/lib\/kubelet\/pki\/kubelet-client-current.pem/
                        s/client-key-data: .*$/client-key: \/var\/lib\/kubelet\/pki\/kubelet-client-current.pem/' /etc/kubernetes/kubelet.conf
        fi
}

ensure_docker_settings() {
        if [ ! -f /etc/docker/daemon.json ]
        then
                echo "Writing Updated Docker Configuration"
                cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "log-driver": "json-file",
  "live-restore": true,
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
                sudo systemctl reload docker
                sudo mkdir -p /etc/systemd/system/docker.service.d
                sudo systemctl daemon-reload
        fi
}

upgrade() {
        ensure_updated_ca

        if [ -f /var/run/docker.sock ]
        then
                ensure_docker_settings
        fi
        #echo "Restarting docker service to ensure correct cgroup"
        #sudo systemctl restart docker
        echo "Kubeadm installed master detected - checking for upgrade"
        sudo apt-get -qq update -y
        sudo apt-get -qq install -y  debconf-utils
        echo "docker.io docker.io/restart select true" | sudo debconf-set-selections
        sudo apt-mark unhold kubeadm
        if ! retry 5 sudo apt-get -qq install -y kubeadm=${kubernetes_release}-00
        then
                sudo apt-mark hold kubeadm
                echo "Failed to install kubeadm at version ${kubernetes_release}-00"
                echo "Version installed is at $(dpkg-query --showformat='$${Version}' --show kubeadm)"
                return 2
        fi
        sudo apt-mark hold kubeadm
        if sudo kubeadm upgrade plan --config $HOME/install/kubeadm.conf | egrep -q '^\s*kubeadm upgrade apply'
        then
                retry 5 sudo kubeadm upgrade apply -y --config $HOME/install/kubeadm.conf
                echo "Upgrading kubelet"
                sudo apt-mark unhold kubelet kubectl
                retry 5 sudo apt-get -qq -y install \
                        kubectl=${kubernetes_release}-00 \
                        kubelet=${kubernetes_release}-00
                sudo apt-mark hold kubelet kubectl
                ensure_certs_in_kubelet_config
                retry 5 sudo systemctl restart kubelet
        else
                echo "Already at version ${kubernetes_release} or greater - skipping"
        fi
        ensure_admin_config
        ensure_ancillary_items
}

if ss -lnt | grep -q :6443
then
        upgrade
else
        install
fi

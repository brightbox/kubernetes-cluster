#!/bin/bash

set -e

install_cert() {
        local source_prefix=$${1}
        local dest_prefix=$${2:-$${1}}
        local target_suffix=$${3:-crt}
        local pki_dir=/etc/kubernetes/pki

        for word in $${target_suffix} key
        do
                sudo cp $${source_prefix}.$${word} $${pki_dir}/$${dest_prefix}.$${word}
        done
        sudo chmod 0600 $${pki_dir}/$${dest_prefix}.key
        rm $${source_prefix}.key
}

# Retries a command on failure.
# $1 - the max number of attempts
# $2... - the command to run
retry() {
    local max_attempts="$${1}"; shift
    local attempt_num=1

    until "$${@}"
    do
        if [ "$${attempt_num}" -eq "$${max_attempts}" ]
        then
            echo "Attempt $${attempt_num} failed and there are no more attempts l
eft!"
            return 1
        else
            echo "Attempt $${attempt_num} failed! Trying again in $${attempt_num}
seconds..."
            sleep $(( attempt_num=attempt_num + 1 ))
        fi
    done
}

install() {
        echo "Installing CA"
        sudo mkdir -p /etc/kubernetes/pki/etcd
        install_cert ca

        echo "Running kubeadm init"
        sudo kubeadm init --config $HOME/install/kubeadm.conf

        ensure_admin_config
        ensure_ancillary_items
}

ensure_admin_config() {
        echo "Installing admin config"
        mkdir -p $HOME/.kube
        sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config

        echo "Installing client cert generator"
        cat <<"CLIENTSCRIPT" | sudo tee /usr/local/bin/generate-client-cert.sh
#!/usr/bin/env bash

if [[ "$${#}" -lt 2 ]]; then
    echo "Usage: $${0} user group [group ...]"
    exit 1
fi

USER=$${1}; shift
GROUP=""
for word
do
  GROUP="$${GROUP}/O=$${word}"
done
CLUSTERENDPOINT=https://${public_fqdn}:6443
CLUSTERNAME=${cluster_name}
CACERT=/etc/kubernetes/pki/ca.crt
CAKEY=/etc/kubernetes/pki/ca.key
CLIENTCERTKEY=clients/$${USER}/$${USER}.key
CLIENTCERTCSR=clients/$${USER}/$${USER}.csr
CLIENTCERTCRT=clients/$${USER}/$${USER}.crt

if [ ! -r "$${CAKEY}" ]; then
    echo "Insufficent permissions to read $${CAKEY}"
    exit 1
fi

mkdir -p clients/$${USER}

openssl genrsa -out $${CLIENTCERTKEY} 4096
openssl req -new -key $${CLIENTCERTKEY} -out $${CLIENTCERTCSR} \
      -subj "/CN=$${USER}$${GROUP}"
openssl x509 -req -days 365 -sha256 -in $${CLIENTCERTCSR} -CA $${CACERT} -CAkey $${CAKEY} -set_serial 2 -out $${CLIENTCERTCRT}

cat <<-EOF > clients/$${USER}/kubeconfig
apiVersion: v1
kind: Config
preferences:
  colors: true
current-context: $${CLUSTERNAME}
clusters:
- name: $${CLUSTERNAME}
  cluster:
    server: $${CLUSTERENDPOINT}
    certificate-authority-data: |-
      $(base64 --wrap=0 $${CACERT})
contexts:
- context:
    cluster: $${CLUSTERNAME}
    user: $${USER}
  name: $${CLUSTERNAME}
users:
- name: $${USER}
  user:
    client-certificate-data: |-
      $(base64 --wrap=0 $${CLIENTCERTCRT})
    client-key-data: |-
      $(base64 --wrap=0 $${CLIENTCERTKEY})
EOF
CLIENTSCRIPT
        echo "Setting permissions"
        sudo chmod 755 /usr/local/bin/generate-client-cert.sh
}

controller_key_name=brightbox-cloud-controller
apiurl_b64=$(echo -n ${apiurl} | base64)
controller_client_b64=$(echo -n ${controller_client} | base64)
controller_client_secret_b64=$(echo -n ${controller_client_secret} | base64)

ensure_ancillary_items() {
        echo "Patching kube-proxy scheduling tolerations"
        if ! kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "spec": { "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/master" } ] } } } }'
        then
                echo "Patch already applied - skipping"
        fi

        echo "Requesting Brightbox cloud controller activation"
        cat <<EOF > $HOME/install/cloud-controller.yml
---
apiVersion: v1
data:
  apiurl: $${apiurl_b64}
  controller-client: $${controller_client_b64}
  controller-client-secret: $${controller_client_secret_b64}
kind: Secret
metadata:
  name: $${controller_key_name}
  namespace: kube-system
type: Opaque
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cloud-controller-manager
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: system:cloud-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: cloud-controller-manager
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
data:
  cloud-controller.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://[${external_ip}]:6443
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    k8s-app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cloud-controller-manager
  template:
    metadata:
      labels:
        k8s-app: cloud-controller-manager
      annotations: {
        "apiclientid": "${controller_client}"
      }
    spec:
      dnsPolicy: Default
      hostNetwork: true
      serviceAccountName: cloud-controller-manager
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
        # this taint is set by all kubelets running '--cloud-provider=external'
        # so we should tolerate it to schedule the brightbox ccm
        - key: "node.cloudprovider.kubernetes.io/uninitialized"
          value: "true"
          effect: "NoSchedule"
        - key: "CriticalAddonsOnly"
          operator: "Exists"
        - key: "node-role.kubernetes.io/master"
          effect: NoSchedule
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
      containers:
      - name: cloud-controller-manager
        # for in-tree providers we use k8s.gcr.io/cloud-controller-manager
        # this can be replaced with any other image for out-of-tree providers
        imagePullPolicy: Always
        image: brightbox/brightbox-cloud-controller-manager:${kubernetes_release}
        args:
          - "--cloud-provider=brightbox"
          - "--bind-address=::1"
          - "--port=0"
          - "--secure-port=10258"
          - "--configure-cloud-routes=false"
          - "--kubeconfig=/etc/kubernetes/cloud-controller.conf"
          - "--cluster-name=${cluster_name}"
          - "--leader-elect=true"
          - --use-service-account-credentials=false
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
        env:
          - name: KUBERNETES_SERVICE_HOST
            value: "${external_ip}"
          - name: KUBERNETES_SERVICE_PORT
            value: "6443"
          - name: BRIGHTBOX_CLIENT
            valueFrom:
              secretKeyRef:
                name: $${controller_key_name}
                key: controller-client
          - name: BRIGHTBOX_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                name: $${controller_key_name}
                key: controller-client-secret
          - name: BRIGHTBOX_API_URL
            valueFrom:
              secretKeyRef:
                name: $${controller_key_name}
                key: apiurl
        volumeMounts:
        - mountPath: /etc/kubernetes/cloud-controller.conf
          name: cloud-controller-conf
          readOnly: true
          subPath: cloud-controller.conf
      hostNetwork: true
      volumes:
      - configMap:
          defaultMode: 420
          name: cloud-controller-manager
        name: cloud-controller-conf
EOF
        retry 5 kubectl apply -f $HOME/install/cloud-controller.yml

        echo "Installing Calico network controllers"
        retry 5 kubectl apply -f \
        https://docs.projectcalico.org/v${calico_release}/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml

        echo "Installing Local Storage Provisioner"
        cat <<EOF > $HOME/install/local-storage-provisioner.yml
---
# Source: provisioner/templates/provisioner.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: kube-system
data:
  storageClassMap: |
    local-storage:
       hostDir: /mnt/disks
       mountDir: /mnt/disks
       blockCleanerCommand:
         - "/scripts/shred.sh"
         - "2"
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: local-volume-provisioner
  namespace: kube-system
  labels:
    app: local-volume-provisioner
spec:
  selector:
    matchLabels:
      app: local-volume-provisioner
  template:
    metadata:
      labels:
        app: local-volume-provisioner
    spec:
      serviceAccountName: local-storage-admin
      containers:
        - image: "quay.io/external_storage/local-volume-provisioner:v2.2.0"
          name: provisioner
          securityContext:
            privileged: true
          resources:
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 50m
              memory: 100Mi
          env:
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: JOB_CONTAINER_IMAGE
            value: "quay.io/external_storage/local-volume-provisioner:v2.2.0"
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            - mountPath: /dev
              name: provisioner-dev
            - mountPath: /mnt/disks
              name: local-storage
              mountPropagation: "HostToContainer"
      volumes:
        - name: provisioner-config
          configMap:
            name: local-provisioner-config
        - name: provisioner-dev
          hostPath:
            path: /dev
        - name: local-storage
          hostPath:
            path: /mnt/disks
---
# Source: provisioner/templates/provisioner-service-account.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-storage-admin
  namespace: kube-system

---
# Source: provisioner/templates/provisioner-cluster-role-binding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-pv-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-storage-provisioner-node-clusterrole
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-node-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: local-storage-provisioner-node-clusterrole
  apiGroup: rbac.authorization.k8s.io
EOF
        retry 5 kubectl apply -f $HOME/install/local-storage-provisioner.yml
}

ensure_docker_settings() {
        if [ ! -f /etc/docker/daemon.json ]
        then
                echo "Writing Updated Docker Configuration"
                cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "log-driver": "json-file",
  "live-restore": true,
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
                sudo systemctl reload docker
                sudo mkdir -p /etc/systemd/system/docker.service.d
                sudo systemctl daemon-reload
        fi
}

upgrade() {
        ensure_docker_settings
        #echo "Restarting docker service to ensure correct cgroup"
        #sudo systemctl restart docker
        echo "Kubeadm installed master detected - checking for upgrade"
        sudo apt-get -qq update -y
        sudo apt-get -qq install debconf-utils
        echo "docker.io docker.io/restart select true" | sudo debconf-set-selections
        sudo apt-get -qq install kubeadm=${kubernetes_release}-00
        if sudo kubeadm upgrade plan --config $HOME/install/kubeadm.conf | egrep '^\s*kubeadm upgrade apply'
        then
                retry 5 sudo kubeadm upgrade apply -y --config $HOME/install/kubeadm.conf
                sudo apt-get -qq install \
                        kubectl=${kubernetes_release}-00 \
                        kubelet=${kubernetes_release}-00
        else
                echo "Already at version ${kubernetes_release} or greater - skipping"
        fi
        ensure_admin_config
        ensure_ancillary_items
}

if sudo kubeadm config view  >/dev/null 2>&1
then
        upgrade
else
        install
fi

#!/bin/bash

set -e

install_cert() {
        local source_prefix=$$1
        local dest_prefix=$${2:-$$1}
        local target_suffix=$${3:-crt}
        local pki_dir=/etc/kubernetes/pki

        for word in $${target_suffix} key
        do
                sudo cp $${source_prefix}.$${word} $${pki_dir}/$${dest_prefix}.$${word}
        done
        sudo chmod 0600 $${pki_dir}/$${dest_prefix}.key
        rm $${source_prefix}.key
}

# Retries a command on failure.
# $1 - the max number of attempts
# $2... - the command to run
retry() {
    local max_attempts="$${1}"; shift
    local attempt_num=1

    until "$${@}"
    do
        if [ "$${attempt_num}" -eq "$${max_attempts}" ]
        then
            echo "Attempt $${attempt_num} failed and there are no more attempts l
eft!"
            return 1
        else
            echo "Attempt $${attempt_num} failed! Trying again in $${attempt_num}
seconds..."
            sleep $(( attempt_num=attempt_num + 1 ))
        fi
    done
}
controller_key_name=brightbox-cloud-controller

install() {
	echo "Installing CA"
	sudo mkdir -p /etc/kubernetes/pki/etcd
	install_cert ca

	echo "Running kubeadm init"
	sudo kubeadm init --config $HOME/install/kubeadm.conf

	ensure_admin_config

	echo "Installing Secrets for Cloud Controller"
	kubectl -n kube-system create secret generic $${controller_key_name} '--from-literal=controller-client=${controller_client}' '--from-literal=controller-client-secret=${controller_client_secret}' '--from-literal=apiurl=${apiurl}'

	ensure_ancillary_items
}

ensure_admin_config() {
	echo "Installing admin config"
	mkdir -p $HOME/.kube
	sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config
}

ensure_ancillary_items() {
	echo "Patching kube-proxy scheduling tolerations"
	kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "spec": { "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/master" } ] } } } }'

	echo "Requesting Brightbox cloud controller activation"
	#cat <<EOF | kubectl apply -f -
	cat <<EOF > $HOME/install/cloud-controller.yml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cloud-controller-manager
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: system:cloud-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: cloud-controller-manager
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
data:
  cloud-controller.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://[${external_ip}]:6443
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    k8s-app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cloud-controller-manager
  template:
    metadata:
      labels:
        k8s-app: cloud-controller-manager
    spec:
      dnsPolicy: Default
      serviceAccountName: cloud-controller-manager
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
        # this taint is set by all kubelets running '--cloud-provider=external'
        # so we should tolerate it to schedule the brightbox ccm
        - key: "node.cloudprovider.kubernetes.io/uninitialized"
          value: "true"
          effect: "NoSchedule"
        - key: "CriticalAddonsOnly"
          operator: "Exists"
        # cloud controller manages should be able to run on masters
        - key: "node-role.kubernetes.io/master"
          effect: NoSchedule
      containers:
      - name: cloud-controller-manager
        # for in-tree providers we use k8s.gcr.io/cloud-controller-manager
        # this can be replaced with any other image for out-of-tree providers
        image: brightbox/brightbox-cloud-controller-manager:${cloud_controller_release}
        command:
          - "/bin/brightbox-cloud-controller-manager"
          - "--cloud-provider=brightbox"
          - "--bind-address=::1"
          - "--port=0"
          - "--secure-port=10253"
          - "--configure-cloud-routes=false"
          - "--kubeconfig=/etc/kubernetes/cloud-controller.conf"
          - "--cluster-name=${cluster_name}"
          - "--leader-elect=true"
          - --use-service-account-credentials=false
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
        env:
          - name: BRIGHTBOX_CLIENT
            valueFrom:
              secretKeyRef:
                name: $${controller_key_name}
                key: controller-client
          - name: BRIGHTBOX_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                name: $${controller_key_name}
                key: controller-client-secret
          - name: BRIGHTBOX_API_URL
            valueFrom:
              secretKeyRef:
                name: $${controller_key_name}
                key: apiurl
        volumeMounts:
        - mountPath: /etc/kubernetes/cloud-controller.conf
          name: cloud-controller-conf
          readOnly: true
          subPath: cloud-controller.conf
      hostNetwork: true
      volumes:
      - configMap:
          defaultMode: 420
          name: cloud-controller-manager
        name: cloud-controller-conf
EOF
	kubectl apply -f $HOME/install/cloud-controller.yml

	echo "Installing Calico network controllers"
	kubectl apply -f \
	https://docs.projectcalico.org/v${calico_release}/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
	kubectl apply -f \
	https://docs.projectcalico.org/v${calico_release}/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml

	echo "Installing Local Storage Provisioner"
	#cat <<EOF | kubectl apply -f -
	cat <<EOF > $HOME/install/local-storage-provisioner.yml
---
# Source: provisioner/templates/provisioner.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: kube-system
data:
  storageClassMap: |
    local-storage:
       hostDir: /mnt/disks
       mountDir: /mnt/disks
       blockCleanerCommand:
         - "/scripts/shred.sh"
         - "2"
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: local-volume-provisioner
  namespace: kube-system
  labels:
    app: local-volume-provisioner
spec:
  selector:
    matchLabels:
      app: local-volume-provisioner
  template:
    metadata:
      labels:
        app: local-volume-provisioner
    spec:
      serviceAccountName: local-storage-admin
      containers:
        - image: "quay.io/external_storage/local-volume-provisioner:v2.2.0"
          name: provisioner
          securityContext:
            privileged: true
          resources:
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 50m
              memory: 100Mi
          env:
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: JOB_CONTAINER_IMAGE
            value: "quay.io/external_storage/local-volume-provisioner:v2.2.0"
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            - mountPath: /dev
              name: provisioner-dev
            - mountPath: /mnt/disks
              name: local-storage
              mountPropagation: "HostToContainer"
      volumes:
        - name: provisioner-config
          configMap:
            name: local-provisioner-config
        - name: provisioner-dev
          hostPath:
            path: /dev
        - name: local-storage
          hostPath:
            path: /mnt/disks
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
# Source: provisioner/templates/provisioner-service-account.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-storage-admin
  namespace: kube-system

---
# Source: provisioner/templates/provisioner-cluster-role-binding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-pv-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-storage-provisioner-node-clusterrole
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-node-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: local-storage-provisioner-node-clusterrole
  apiGroup: rbac.authorization.k8s.io
EOF
	kubectl apply -f $HOME/install/local-storage-provisioner.yml
}

upgrade() {
	echo "Kubeadm installed master detected - checking for upgrade"
	if sudo kubeadm upgrade plan --config $HOME/install/kubeadm.conf | egrep '^\s*kubeadm upgrade apply'
	then
		sudo apt-get -qq update -y
		sudo apt-get -qq install kubeadm=${kubernetes_release}-00 
		retry 5 sudo kubeadm upgrade apply -y --config $HOME/install/kubeadm.conf
		sudo apt-get -qq install \
			kubectl=${kubernetes_release}-00 \
			kubelet=${kubernetes_release}-00
	else
		echo "Already at version ${kubernetes_release} or greater - skipping"
	fi
	ensure_admin_config
	ensure_ancillary_items
}

if sudo kubeadm config view  >/dev/null 2>&1
then
	upgrade
else
	install
fi
